{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN for the original whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\minij\\anaconda3\\envs\\special_course\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import of required libraries and functions from 'make_dataset' script\n",
    "import os\n",
    "from make_dataset import Discotope_Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, log_loss, confusion_matrix, roc_auc_score\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Importing the style package\n",
    "from matplotlib import style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tensorflow and keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_load(name_set, data_dir, separate=False):\n",
    "    \n",
    "    '''\n",
    "    Function to load training, validation or test data from the folder/directory you have storaged the whole dataset\n",
    "        - 'name_set': text variable with the type of data set to load ('train', 'test', 'valid')\n",
    "        - 'data_dir': directory where you have storaged the data set (in my case is '../Data/')\n",
    "        - 'separate': boolean that returns the PDBs and AF2 sets separately (only when True)\n",
    "    \n",
    "    Output:\n",
    "        - 'X_all': all observations from both solved and predicted structures all together\n",
    "        - 'y_all': all labels from both solved and predicted structures all together\n",
    "        - 'N_solved': number of observations from solved structures (in case of wanted to unmerge the 'all' array)\n",
    "        - 'N_af2': number of observations from predicted structures (in case of wanted to unmerge the 'all' array)\n",
    "    '''\n",
    "    \n",
    "    # 'Pathlib module' load to work with windows path\n",
    "    import pathlib\n",
    "    temp = pathlib.PosixPath\n",
    "    pathlib.PosixPath = pathlib.WindowsPath\n",
    "    \n",
    "    # List of all subdirectories inside the data directory\n",
    "    dirs = [d for d in os.listdir(data_dir)]\n",
    "    \n",
    "    # 're' module to use regex for filtering the specific directories according to the type of data set to load\n",
    "    pattern = '.*' + name_set + '.*'\n",
    "    R = re.compile(pattern)\n",
    "    filtered = [folder for folder in dirs if R.match(folder)]\n",
    "    \n",
    "    # Loading the data set for solved structures and AlphaFold2 predicted structures\n",
    "    path_af2 = data_dir + filtered[0] + '/dataset.pt'\n",
    "    path_solved = data_dir + filtered[1] + '/dataset.pt'\n",
    "    set_af2 = torch.load(path_af2)\n",
    "    set_solved = torch.load(path_solved)\n",
    "    \n",
    "    # Stack all features and targets to one big array (removing NaN entries)\n",
    "    X_set_solved = np.concatenate([set_solved[i][\"X_arr\"] for i in range(0, len(set_solved), 1) if not set_solved[i]['df_stats']['rsa'].isna().any()])\n",
    "    y_set_solved = np.concatenate([set_solved[i][\"y_arr\"] for i in range(0, len(set_solved), 1) if not set_solved[i]['df_stats']['rsa'].isna().any()])\n",
    "    X_set_af2 = np.concatenate([set_af2[i][\"X_arr\"] for i in range(0, len(set_solved), 1) if not set_solved[i]['df_stats']['rsa'].isna().any()])\n",
    "    y_set_af2 = np.concatenate([set_af2[i][\"y_arr\"] for i in range(0, len(set_solved), 1) if not set_solved[i]['df_stats']['rsa'].isna().any()])\n",
    "    \n",
    "    # NÂº of observations for each subtype of data set\n",
    "    N_set_solved = X_set_solved.shape[0]\n",
    "    N_set_af2 = X_set_af2.shape[0]\n",
    "    \n",
    "    # Stack all features and targets from solved and predicted structures into only one big\n",
    "    X_set_all = np.concatenate((X_set_solved, X_set_af2), axis=0)\n",
    "    y_set_all = np.concatenate((y_set_solved, y_set_af2), axis=0)\n",
    "    \n",
    "    if (separate==True):\n",
    "        return(X_set_all, y_set_all, X_set_solved, y_set_solved, X_set_af2, y_set_af2)\n",
    "    else:\n",
    "        return(X_set_all, y_set_all, N_set_solved, N_set_af2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_df_stats_creation(name_set, data_dir):\n",
    "    \n",
    "    '''\n",
    "    Function to store the stats information from each pdb of the original dataset\n",
    "        - 'name_set': text variable with the type of data set to load ('train', 'test', 'valid')\n",
    "        - 'data_dir': directory where you have storaged the data set (in my case is '../Data/')\n",
    "    \n",
    "    Output:\n",
    "        - 'stats_df_solved': dataframe for the solved structures (removing NaN entries of RSA values)\n",
    "        - 'stats_df_af2': dataframe for the AF2 structures (removing NaN entries of RSA values)\n",
    "    '''\n",
    "    \n",
    "    # 'Pathlib module' load to work with windows path\n",
    "    import pathlib\n",
    "    temp = pathlib.PosixPath\n",
    "    pathlib.PosixPath = pathlib.WindowsPath\n",
    "    \n",
    "    # List of all subdirectories inside the data directory\n",
    "    dirs = [d for d in os.listdir(data_dir)]\n",
    "    \n",
    "    # 're' module to use regex for filtering the specific directories according to the type of data set to load\n",
    "    pattern = '.*' + name_set + '.*'\n",
    "    R = re.compile(pattern)\n",
    "    filtered = [folder for folder in dirs if R.match(folder)]\n",
    "    \n",
    "    # Loading the data set for solved structures and AlphaFold2 predicted structures\n",
    "    path_af2 = data_dir + filtered[0] + '/dataset.pt'\n",
    "    path_solved = data_dir + filtered[1] + '/dataset.pt'\n",
    "    set_af2 = torch.load(path_af2)\n",
    "    set_solved = torch.load(path_solved)\n",
    "    \n",
    "    # Stats dataframe creation for SOLVED and AF2 structures (removing 'NaN' entries according the PDB set)\n",
    "    stats_dfs_solved = []\n",
    "    stats_dfs_af2 = []\n",
    "    for i in range(0, len(set_solved), 1):\n",
    "        sample = set_solved[i]\n",
    "        sample_af2 = set_af2[i]\n",
    "        df_sample = sample['df_stats']\n",
    "        df_sample_af2 = sample_af2['df_stats']\n",
    "        \n",
    "        # Removing 'NaN' entries and adding the epitope column\n",
    "        if(df_sample['rsa'].isna().any() == False):\n",
    "            df_sample['epitope'] = sample['y_arr'].astype(bool)\n",
    "            df_sample_af2['epitope'] = sample_af2['y_arr'].astype(bool)\n",
    "            stats_dfs_solved.append(df_sample)\n",
    "            stats_dfs_af2.append(df_sample_af2)\n",
    "    \n",
    "    return(stats_dfs_solved, stats_dfs_af2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_load(name_set, data_dir):\n",
    "    \n",
    "    '''\n",
    "    Function to load training, validation or test dataframes from the folder/directory you have storaged the whole dataset.\n",
    "    This function is specifically to have the original dataframes of the data, and their corresponding description\n",
    "        - 'name_set': text variable with the type of data set to load ('train', 'test', 'valid')\n",
    "        - 'data_dir': directory where you have storaged the data set (in my case is '../Data/')\n",
    "    \n",
    "    Output:\n",
    "        - 'set_af2': dataframe for AF2 predicted structures\n",
    "        - 'set_solved': dataframe for PDB solved structures\n",
    "    '''\n",
    "    \n",
    "    # 'Pathlib module' load to work with windows path\n",
    "    import pathlib\n",
    "    temp = pathlib.PosixPath\n",
    "    pathlib.PosixPath = pathlib.WindowsPath\n",
    "    \n",
    "    # List of all subdirectories inside the data directory\n",
    "    dirs = [d for d in os.listdir(data_dir)]\n",
    "    \n",
    "    # 're' module to use regex for filtering the specific directories according to the type of data set to load\n",
    "    pattern = '.*' + name_set + '.*'\n",
    "    R = re.compile(pattern)\n",
    "    filtered = [folder for folder in dirs if R.match(folder)]\n",
    "    \n",
    "    # Loading the data set for solved structures and AlphaFold2 predicted structures\n",
    "    path_af2 = data_dir + filtered[0] + '/dataset.pt'\n",
    "    path_solved = data_dir + filtered[1] + '/dataset.pt'\n",
    "    set_af2 = torch.load(path_af2)\n",
    "    set_solved = torch.load(path_solved)\n",
    "    \n",
    "    return(set_af2, set_solved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_NaN(data, y):\n",
    "    \n",
    "    '''\n",
    "    Function to remove NaN values (some PDB entries have RSA NaN values)\n",
    "        - 'data': numpy array with the specific (train, valid, test) data\n",
    "        - 'y': numpy array with the specific (train, valid, test) labels\n",
    "    \n",
    "    Output:\n",
    "        - 'data_noNaN': array withouth the entries/observations that contain NaN values\n",
    "    '''\n",
    "    \n",
    "    # Merging X and y arrays all together\n",
    "    joint_data = np.hstack((data, y.reshape(-1, 1)))\n",
    "    \n",
    "    # Removal of NaN entries\n",
    "    nan_rows = np.isnan(joint_data).any(axis=1)\n",
    "    data_noNaN = joint_data[~nan_rows, :]\n",
    "    \n",
    "    # Demerging the final array into X and y\n",
    "    X_noNaN = data_noNaN[:, 0:data.shape[1]]\n",
    "    y_noNaN = data_noNaN[:,-1]\n",
    "    \n",
    "    return(X_noNaN, y_noNaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_train(X):\n",
    "    \n",
    "    '''\n",
    "    Function to normalize the columns 532 (pLLDT) and 533 (length) because they have high length\n",
    "        - 'X': data to normalize\n",
    "    '''\n",
    "    \n",
    "    # Create a copy of the X vector to do the normalization\n",
    "    X_scaled = X.copy()\n",
    "    \n",
    "    # Create an instance of MinMaxScaler\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Fit the scaler to the data \n",
    "    #scaler.fit(X_scaled[:, 532:534])\n",
    "    scaler.fit(X_scaled)\n",
    "    \n",
    "    # Transform the data\n",
    "    #X_scaled[:, 532:534] = scaler.fit_transform(X_scaled[:, 532:534])\n",
    "    X_scaled = scaler.fit_transform(X_scaled)\n",
    "    return(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Z_transform_train(X, cols_to_select):\n",
    "    \n",
    "    '''\n",
    "    Function to normalize the columns 532 (pLLDT) and 533 (length) because they have high length\n",
    "        - 'X': data to normalize\n",
    "    '''\n",
    "    \n",
    "    # Obtain the mean and standard deviation for each feature on the array\n",
    "    X_mean = np.mean(X[:, cols_to_select], axis=0)\n",
    "    X_std = np.std(X[:, cols_to_select], axis=0)\n",
    "    \n",
    "    # Correcting pLDDT column sd in case of using only PDB set\n",
    "    X_std[X_std == 0] = 1\n",
    "    \n",
    "    # Z-transform (standardization)\n",
    "    X[:, cols_to_select] = (X[:, cols_to_select] - X_mean)/X_std\n",
    "    return(X, X_mean, X_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Z_transform_valid(X, mean_train, sd_train, cols_to_select):\n",
    "    \n",
    "    '''\n",
    "    Function to normalize the columns 532 (pLLDT) and 533 (length) because they have high length\n",
    "        - 'X': data to normalize\n",
    "        - 'mean_train': mean from standardized training set\n",
    "        - 'sd_train': standard deviation from standardized training set\n",
    "    '''\n",
    "    \n",
    "    # Z-transform (standardization)\n",
    "    X[:, cols_to_select] = (X[:, cols_to_select] - mean_train)/sd_train\n",
    "    return(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_weight_calculator(y_train):\n",
    "        \n",
    "    '''\n",
    "    Function to calculate the class weights for the unbalanced data\n",
    "        - 'y_train': training labels (contains 0 and 1)\n",
    "    '''\n",
    "    \n",
    "    # Compute the class weights with sklearn function\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "\n",
    "    # Convert the class weights to a dictionary\n",
    "    class_weight = dict(enumerate(class_weights))\n",
    "    return(class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neuronal network model with one layer\n",
    "def nn_model(train_data, y_train, valid_data, y_valid, act_fun, loss_fun, h, alpha, \n",
    "             class_weight_fn, drop_rate, batch = None, balancing=True):\n",
    "    \n",
    "    '''\n",
    "    Function to create and train/validate the feed-forward neuronal network with only 1 hidden layer\n",
    "        -'train_data': X train standardized\n",
    "        -'y_train': training labels\n",
    "        -'act_fun': activation function\n",
    "        -'loss_fun': loss function\n",
    "        -'h': number of hidden units\n",
    "        -'alpha': L2 regularization value\n",
    "        -'class_weight_calculator': function to calculate the weights for each class\n",
    "        -'batch': batch_size (baseline is None, but can be changed by adding a integer as new value)\n",
    "        -'balancing': True/False argument for incorporating balancing in classes\n",
    "    \n",
    "    Output:\n",
    "        -'model': neural network model trained\n",
    "        -'history': attributes obtained during fitting the model\n",
    "    '''\n",
    "    \n",
    "    # Calculation of the class weights with function previously defined\n",
    "    class_weight = class_weight_fn(y_train)\n",
    "    \n",
    "    # Normalization of the class_weight to sum 1\n",
    "    tot = class_weight[0] + class_weight[1]\n",
    "    class_weight[0] = class_weight[0]/tot\n",
    "    class_weight[1] = class_weight[1]/tot\n",
    "    \n",
    "    # Implementation of keras for creating a sequential model with 1 layer\n",
    "    tf.random.set_seed(1234)\n",
    "    from keras.layers import Dense, Dropout\n",
    "    from keras import regularizers, metrics\n",
    "    \n",
    "    model = keras.Sequential()\n",
    "    # Input layer with train_data.shape neurons and a hidden layer with 1 neuron\n",
    "    model.add(Dense(h, activation=act_fun, input_shape=train_data.shape[1:], kernel_regularizer=regularizers.l2(alpha)))\n",
    "    model.add(Dropout(drop_rate))\n",
    "    # Output layer with sigmoid activation (better for binary classification)\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    model.compile(optimizer=opt, loss=loss_fun, metrics=\n",
    "                  ['accuracy', metrics.Precision(), metrics.Recall(), metrics.AUC(), loss_fun])\n",
    "    \n",
    "    if (balancing == True):\n",
    "        history = model.fit(train_data, y_train, epochs = 100, batch_size=batch, verbose=0, class_weight=class_weight, \n",
    "                            validation_data = (valid_data, y_valid))\n",
    "    else:\n",
    "        history = model.fit(train_data, y_train, epochs = 100, batch_size=batch, verbose=0, \n",
    "                            validation_data = (valid_data, y_valid))\n",
    "    \n",
    "    return(model, history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_plot(history, h_units, l2, drop):\n",
    "    \n",
    "    '''\n",
    "    Function to plot the loss curve of the training of the model\n",
    "        - 'loss_values': array with the loss values for each iteration of the training\n",
    "    '''\n",
    "    \n",
    "    plt.plot(history.history['loss'], label = 'train')\n",
    "    plt.plot(history.history['val_loss'], label='validation')\n",
    "    plt.title('Model loss when trying ' + str(h_units) + ' hunits,\\n' + str(l2) + ' reg, and ' + str(drop) + ' dropout')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(loc='best')\n",
    "    \n",
    "    plt.savefig('Loss - ' + str(h_units) + ' hunits, ' + str(l2) + ' reg, and ' + str(drop) + ' drop.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ROC_AUC(y_true, y_hat):\n",
    "    \n",
    "    '''\n",
    "    Function to obtain the AUC value based on the ROC curve\n",
    "        - 'y_true': y original values\n",
    "        - 'y_hat': y predicted values\n",
    "        \n",
    "    Output:\n",
    "        - 'ROC_auc': AUC value\n",
    "    '''\n",
    "    \n",
    "    fpr, tpr, _ = metrics.roc_curve(y_true, y_hat)\n",
    "    ROC_auc = metrics.auc(fpr, tpr)\n",
    "    return(ROC_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PR_AUC(y_true, y_hat):\n",
    "    \n",
    "    '''\n",
    "    Function to obtain the AUC value based on the precision and recall parameters\n",
    "        - 'y_true': y original values\n",
    "        - 'y_hat': y predicted values\n",
    "        \n",
    "    Output:\n",
    "        - 'PR_auc': AUC value\n",
    "    '''\n",
    "    \n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y_true, y_hat)\n",
    "    PR_auc = metrics.auc(recall, precision)\n",
    "    return(PR_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epitope rank percentile score\n",
    "# https://github.com/Magnushhoie/discotope3/blob/main/src/models/mlscripts.py#L55\n",
    "\n",
    "def get_percentile_score_arr(\n",
    "    scores: np.array,\n",
    "    epitopes: np.array,\n",
    "):\n",
    "    \n",
    "    \"\"\"Find mean predicted epitope rank percentile score from the scores (y_hat) and the epitopes (y_true)\"\"\"\n",
    "    epitopes_bool = epitopes.astype(bool)\n",
    "    assert epitopes_bool.dtype == \"bool\"\n",
    "\n",
    "    c = scores[epitopes_bool].mean()\n",
    "    c_percentile = (c > scores).mean()\n",
    "\n",
    "    return c_percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((446835, 536), (446835,), (115978, 536), (115978,), (11882, 536), (11882,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data loading for training, validation, and test data sets (needs a couple of minutes)\n",
    "X_train, y_train, X_train_PDB, y_train_PDB, X_train_af2, y_train_af2 = data_load(name_set='train', data_dir='../Data/', separate = True)\n",
    "X_valid, y_valid, X_valid_PDB, y_valid_PDB, X_valid_af2, y_valid_af2 = data_load(name_set='valid', data_dir='../Data/', separate = True)\n",
    "X_test, y_test, X_test_PDB, y_test_PDB, X_test_af2, y_test_af2 = data_load(name_set='test', data_dir='../Data/', separate = True)\n",
    "\n",
    "X_train.shape, y_train.shape, X_valid.shape, y_valid.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## NOT NEEDED: ALREADY REMOVED WHEN LOADING THE DATA\n",
    "\n",
    "# # Data manipulation to remove all PDB entries with NaN values in the RSA feature (535)\n",
    "# X_train, y_train = remove_NaN(data=X_train, y=y_train)\n",
    "# X_valid, y_valid = remove_NaN(data=X_valid, y=y_valid)\n",
    "# X_test, y_test = remove_NaN(data=X_test, y=y_test)\n",
    "\n",
    "# X_train.shape, y_train.shape, X_valid.shape, y_valid.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((446835, 536), (115978, 536), (11882, 536))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data normalization\n",
    "cols = list(range(0, 512)) + [532] + [533] + [535]\n",
    "X_train_sc, mean_X_train, sd_X_train = Z_transform_train(X_train, cols_to_select=cols)\n",
    "X_valid_sc = Z_transform_valid(X_valid, mean_X_train, sd_X_train, cols_to_select=cols)\n",
    "X_test_sc = Z_transform_valid(X_test, mean_X_train, sd_X_train, cols_to_select=cols)\n",
    "\n",
    "X_train_sc.shape, X_valid_sc.shape, X_test_sc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((223346, 536), (57989, 536), (5941, 536))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data normalization (PDB only)\n",
    "cols = list(range(0, 512)) + [532] + [533] + [535]\n",
    "X_train_PDB_sc, mean_X_train_PDB, sd_X_train_PDB = Z_transform_train(X_train_PDB, cols_to_select=cols)\n",
    "X_valid_PDB_sc = Z_transform_valid(X_valid_PDB, mean_X_train_PDB, sd_X_train_PDB, cols_to_select=cols)\n",
    "X_test_PDB_sc = Z_transform_valid(X_test_PDB, mean_X_train_PDB, sd_X_train_PDB, cols_to_select=cols)\n",
    "\n",
    "X_train_PDB_sc.shape, X_valid_PDB_sc.shape, X_test_PDB_sc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((223489, 536), (57989, 536), (5941, 536))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data normalization (AF2 only)\n",
    "cols = list(range(0, 512)) + [532] + [533] + [535]\n",
    "X_train_af2_sc, mean_X_train_af2, sd_X_train_af2 = Z_transform_train(X_train_af2, cols_to_select=cols)\n",
    "X_valid_af2_sc = Z_transform_valid(X_valid_af2, mean_X_train_af2, sd_X_train_af2, cols_to_select=cols)\n",
    "X_test_af2_sc = Z_transform_valid(X_test_af2, mean_X_train_af2, sd_X_train_af2, cols_to_select=cols)\n",
    "\n",
    "X_train_af2_sc.shape, X_valid_af2_sc.shape, X_test_af2_sc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class count (train):\n",
      "\n",
      "      Total: 446835\n",
      "      Epitope label: 40938 (9.16% of total)\n"
     ]
    }
   ],
   "source": [
    "# Class unbalanced in train\n",
    "zero, one = np.bincount(y_train.astype(int))\n",
    "total = zero + one\n",
    "print(\"Class count (train):\\n\\n      Total: {}\\n      Epitope label: {} ({:.2f}% of total)\".format(total, one, 100*one/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class count (validation):\n",
      "\n",
      "      Total: 115978\n",
      "      Epitope label: 9578 (8.26% of total)\n"
     ]
    }
   ],
   "source": [
    "# Class unbalanced in valid\n",
    "zero, one = np.bincount(y_valid.astype(int))\n",
    "total = zero + one\n",
    "print(\"Class count (validation):\\n\\n      Total: {}\\n      Epitope label: {} ({:.2f}% of total)\".format(total, one, 100*one/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class count (test):\n",
      "\n",
      "      Total: 11882\n",
      "      Epitope label: 712 (5.99% of total)\n"
     ]
    }
   ],
   "source": [
    "# Class unbalanced in train\n",
    "zero, one = np.bincount(y_test.astype(int))\n",
    "total = zero + one\n",
    "print(\"Class count (test):\\n\\n      Total: {}\\n      Epitope label: {} ({:.2f}% of total)\".format(total, one, 100*one/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weight implemented\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 0.09161771123569103, 1: 0.9083822887643089}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculation of the class weights with function previously defined\n",
    "class_weight = class_weight_calculator(y_train)\n",
    "    \n",
    "# Normalization of the class_weight to sum 1\n",
    "tot = class_weight[0] + class_weight[1]\n",
    "class_weight[0] = class_weight[0]/tot\n",
    "class_weight[1] = class_weight[1]/tot\n",
    "    \n",
    "print(\"Class weight implemented\")\n",
    "class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop trying different possible combinations of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Parameters definition\n",
    "loss = 'binary_crossentropy'\n",
    "act = 'relu'\n",
    "l2_value = [1e-4, 1e-3, 1e-2, 1e-1, 1]\n",
    "batch_size = 128\n",
    "class_balancing = False\n",
    "drop_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "# Hidden units to try\n",
    "h_units = [1, 2, 5, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text file with results\n",
    "file_name = 'AUC_hunits_hyperparams.txt'\n",
    "file = open(file_name, \"a\")\n",
    "file.write('H_units' + '\\t' + 'L2_reg' + '\\t' + 'dropout_rate' + '\\t' + 'AUC_valid' + '\\t' + 'AUC_valid_PDB' + '\\t' + 'AUC_valid_af2' + '\\t' + 'AUC_test' + '\\t' + 'AUC_test_PDB' + '\\t' + 'AUC_test_af2' + '\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished try with 5 hidden units, 1 L2 reg, and  0.0 dropout\n",
      "Finished try with 5 hidden units, 1 L2 reg, and  0.1 dropout\n",
      "Finished try with 5 hidden units, 1 L2 reg, and  0.2 dropout\n",
      "Finished try with 5 hidden units, 1 L2 reg, and  0.3 dropout\n",
      "Finished try with 5 hidden units, 1 L2 reg, and  0.4 dropout\n",
      "Finished try with 5 hidden units, 1 L2 reg, and  0.5 dropout\n",
      "Finished try with 10 hidden units, 1 L2 reg, and  0.0 dropout\n",
      "Finished try with 10 hidden units, 1 L2 reg, and  0.1 dropout\n",
      "Finished try with 10 hidden units, 1 L2 reg, and  0.2 dropout\n",
      "Finished try with 10 hidden units, 1 L2 reg, and  0.3 dropout\n",
      "Finished try with 10 hidden units, 1 L2 reg, and  0.4 dropout\n",
      "Finished try with 10 hidden units, 1 L2 reg, and  0.5 dropout\n"
     ]
    }
   ],
   "source": [
    "# Manual loop for parameter combinations\n",
    "for hunit in h_units:\n",
    "    \n",
    "    for l2_reg in l2_value:\n",
    "        \n",
    "        for drop in drop_rate:\n",
    "    \n",
    "            # Training and validation of the model\n",
    "            model, history = nn_model(train_data=X_train_sc, y_train=y_train, valid_data=X_valid_sc, y_valid=y_valid, \n",
    "                                      act_fun=act, loss_fun=loss, h=hunit, alpha=l2_reg, class_weight_fn=class_weight_calculator,\n",
    "                                      drop_rate = drop, batch = batch_size, balancing = class_balancing)    \n",
    "\n",
    "            # Obtaining probabilities values\n",
    "            y_pred_valid_prob = model.predict(X_valid_sc, verbose=0)\n",
    "            y_pred_test_prob = model.predict(X_test_sc, verbose=0)\n",
    "\n",
    "            # Obtaining probabilities values\n",
    "            y_pred_valid_PDB_prob = model.predict(X_valid_PDB_sc, verbose=0)\n",
    "            y_pred_test_PDB_prob = model.predict(X_test_PDB_sc, verbose=0)\n",
    "\n",
    "            # Obtaining probabilities values\n",
    "            y_pred_valid_af2_prob = model.predict(X_valid_af2_sc, verbose=0)\n",
    "            y_pred_test_af2_prob = model.predict(X_test_af2_sc, verbose=0)\n",
    "\n",
    "            # ROC-AUC value (valid)\n",
    "            ROC_AUC_valid = ROC_AUC(y_valid, y_pred_valid_prob)\n",
    "            ROC_AUC_valid_PDB = ROC_AUC(y_valid_PDB, y_pred_valid_PDB_prob)\n",
    "            ROC_AUC_valid_af2 = ROC_AUC(y_valid_af2, y_pred_valid_af2_prob)\n",
    "            # ROC-AUC value (test)\n",
    "            ROC_AUC_test = ROC_AUC(y_test, y_pred_test_prob)\n",
    "            ROC_AUC_test_PDB = ROC_AUC(y_test_PDB, y_pred_test_PDB_prob)\n",
    "            ROC_AUC_test_af2 = ROC_AUC(y_test_af2, y_pred_test_af2_prob)\n",
    "\n",
    "            # Text file with results\n",
    "            file = open(file_name, \"a\")\n",
    "            word_h = str(hunit) + '_unit'\n",
    "            file.write(word_h + '\\t' + str(l2_reg) + '\\t' + str(drop) + '\\t' +\n",
    "                       str(ROC_AUC_valid) + '\\t' + str(ROC_AUC_valid_PDB) + '\\t' + str(ROC_AUC_valid_af2) + \n",
    "                       '\\t' + str(ROC_AUC_test) + '\\t' + str(ROC_AUC_test_PDB) + '\\t' + str(ROC_AUC_test_af2) + '\\n')\n",
    "            file.close()\n",
    "\n",
    "            # Save training/validation loss plot\n",
    "            loss_plot(history, hunit, l2_reg, drop)\n",
    "\n",
    "            # Print round\n",
    "            print(\"Finished try with\", str(hunit), \"hidden units,\", str(l2_reg), \"L2 reg, and \", str(drop), \"dropout\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>H_units</th>\n",
       "      <th>L2_reg</th>\n",
       "      <th>dropout_rate</th>\n",
       "      <th>AUC_valid</th>\n",
       "      <th>AUC_valid_PDB</th>\n",
       "      <th>AUC_valid_af2</th>\n",
       "      <th>AUC_test</th>\n",
       "      <th>AUC_test_PDB</th>\n",
       "      <th>AUC_test_af2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>10_unit</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.794642</td>\n",
       "      <td>0.802523</td>\n",
       "      <td>0.786897</td>\n",
       "      <td>0.780097</td>\n",
       "      <td>0.785078</td>\n",
       "      <td>0.775623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>10_unit</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.793855</td>\n",
       "      <td>0.799442</td>\n",
       "      <td>0.789058</td>\n",
       "      <td>0.774642</td>\n",
       "      <td>0.780682</td>\n",
       "      <td>0.769971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>10_unit</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.792883</td>\n",
       "      <td>0.798456</td>\n",
       "      <td>0.787688</td>\n",
       "      <td>0.772730</td>\n",
       "      <td>0.781825</td>\n",
       "      <td>0.763755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>10_unit</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.791244</td>\n",
       "      <td>0.798602</td>\n",
       "      <td>0.783384</td>\n",
       "      <td>0.783574</td>\n",
       "      <td>0.790344</td>\n",
       "      <td>0.776481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>10_unit</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.790875</td>\n",
       "      <td>0.796170</td>\n",
       "      <td>0.785691</td>\n",
       "      <td>0.774602</td>\n",
       "      <td>0.784307</td>\n",
       "      <td>0.766660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>10_unit</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.790723</td>\n",
       "      <td>0.796635</td>\n",
       "      <td>0.785012</td>\n",
       "      <td>0.771631</td>\n",
       "      <td>0.782724</td>\n",
       "      <td>0.760727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>10_unit</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.790420</td>\n",
       "      <td>0.796818</td>\n",
       "      <td>0.783857</td>\n",
       "      <td>0.777153</td>\n",
       "      <td>0.783009</td>\n",
       "      <td>0.772751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>10_unit</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.790375</td>\n",
       "      <td>0.796361</td>\n",
       "      <td>0.783967</td>\n",
       "      <td>0.772114</td>\n",
       "      <td>0.781452</td>\n",
       "      <td>0.764700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>10_unit</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.789449</td>\n",
       "      <td>0.797259</td>\n",
       "      <td>0.782122</td>\n",
       "      <td>0.781314</td>\n",
       "      <td>0.787629</td>\n",
       "      <td>0.776113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>10_unit</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.789414</td>\n",
       "      <td>0.794942</td>\n",
       "      <td>0.782269</td>\n",
       "      <td>0.763114</td>\n",
       "      <td>0.775033</td>\n",
       "      <td>0.752789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>10_unit</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.789251</td>\n",
       "      <td>0.796909</td>\n",
       "      <td>0.781015</td>\n",
       "      <td>0.771311</td>\n",
       "      <td>0.784462</td>\n",
       "      <td>0.757575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>5_unit</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.788135</td>\n",
       "      <td>0.796903</td>\n",
       "      <td>0.779455</td>\n",
       "      <td>0.783808</td>\n",
       "      <td>0.789620</td>\n",
       "      <td>0.778691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>5_unit</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.787927</td>\n",
       "      <td>0.795865</td>\n",
       "      <td>0.780231</td>\n",
       "      <td>0.785527</td>\n",
       "      <td>0.788068</td>\n",
       "      <td>0.784689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>5_unit</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.787313</td>\n",
       "      <td>0.795778</td>\n",
       "      <td>0.779536</td>\n",
       "      <td>0.775227</td>\n",
       "      <td>0.781644</td>\n",
       "      <td>0.769817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>5_unit</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.787221</td>\n",
       "      <td>0.794948</td>\n",
       "      <td>0.779464</td>\n",
       "      <td>0.778828</td>\n",
       "      <td>0.785461</td>\n",
       "      <td>0.773256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>5_unit</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.787058</td>\n",
       "      <td>0.795293</td>\n",
       "      <td>0.779181</td>\n",
       "      <td>0.784023</td>\n",
       "      <td>0.790563</td>\n",
       "      <td>0.779422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>10_unit</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.786179</td>\n",
       "      <td>0.791884</td>\n",
       "      <td>0.779926</td>\n",
       "      <td>0.771484</td>\n",
       "      <td>0.781711</td>\n",
       "      <td>0.762341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>5_unit</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.786144</td>\n",
       "      <td>0.792346</td>\n",
       "      <td>0.780247</td>\n",
       "      <td>0.771805</td>\n",
       "      <td>0.777102</td>\n",
       "      <td>0.769451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>10_unit</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.785611</td>\n",
       "      <td>0.794361</td>\n",
       "      <td>0.776351</td>\n",
       "      <td>0.774245</td>\n",
       "      <td>0.780995</td>\n",
       "      <td>0.766844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>5_unit</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.785500</td>\n",
       "      <td>0.793300</td>\n",
       "      <td>0.777677</td>\n",
       "      <td>0.782006</td>\n",
       "      <td>0.788225</td>\n",
       "      <td>0.777999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     H_units  L2_reg  dropout_rate  AUC_valid  AUC_valid_PDB  AUC_valid_af2  \\\n",
       "100  10_unit  0.0100           0.4   0.794642       0.802523       0.786897   \n",
       "89   10_unit  0.0001           0.5   0.793855       0.799442       0.789058   \n",
       "87   10_unit  0.0001           0.3   0.792883       0.798456       0.787688   \n",
       "95   10_unit  0.0010           0.5   0.791244       0.798602       0.783384   \n",
       "93   10_unit  0.0010           0.3   0.790875       0.796170       0.785691   \n",
       "94   10_unit  0.0010           0.4   0.790723       0.796635       0.785012   \n",
       "88   10_unit  0.0001           0.4   0.790420       0.796818       0.783857   \n",
       "99   10_unit  0.0100           0.3   0.790375       0.796361       0.783967   \n",
       "101  10_unit  0.0100           0.5   0.789449       0.797259       0.782122   \n",
       "97   10_unit  0.0100           0.1   0.789414       0.794942       0.782269   \n",
       "98   10_unit  0.0100           0.2   0.789251       0.796909       0.781015   \n",
       "76    5_unit  0.0100           0.4   0.788135       0.796903       0.779455   \n",
       "71    5_unit  0.0010           0.5   0.787927       0.795865       0.780231   \n",
       "64    5_unit  0.0001           0.4   0.787313       0.795778       0.779536   \n",
       "77    5_unit  0.0100           0.5   0.787221       0.794948       0.779464   \n",
       "74    5_unit  0.0100           0.2   0.787058       0.795293       0.779181   \n",
       "92   10_unit  0.0010           0.2   0.786179       0.791884       0.779926   \n",
       "69    5_unit  0.0010           0.3   0.786144       0.792346       0.780247   \n",
       "102  10_unit  0.1000           0.0   0.785611       0.794361       0.776351   \n",
       "75    5_unit  0.0100           0.3   0.785500       0.793300       0.777677   \n",
       "\n",
       "     AUC_test  AUC_test_PDB  AUC_test_af2  \n",
       "100  0.780097      0.785078      0.775623  \n",
       "89   0.774642      0.780682      0.769971  \n",
       "87   0.772730      0.781825      0.763755  \n",
       "95   0.783574      0.790344      0.776481  \n",
       "93   0.774602      0.784307      0.766660  \n",
       "94   0.771631      0.782724      0.760727  \n",
       "88   0.777153      0.783009      0.772751  \n",
       "99   0.772114      0.781452      0.764700  \n",
       "101  0.781314      0.787629      0.776113  \n",
       "97   0.763114      0.775033      0.752789  \n",
       "98   0.771311      0.784462      0.757575  \n",
       "76   0.783808      0.789620      0.778691  \n",
       "71   0.785527      0.788068      0.784689  \n",
       "64   0.775227      0.781644      0.769817  \n",
       "77   0.778828      0.785461      0.773256  \n",
       "74   0.784023      0.790563      0.779422  \n",
       "92   0.771484      0.781711      0.762341  \n",
       "69   0.771805      0.777102      0.769451  \n",
       "102  0.774245      0.780995      0.766844  \n",
       "75   0.782006      0.788225      0.777999  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in tab-separated file into DataFrame\n",
    "df = pd.read_csv(file_name, sep='\\t')\n",
    "\n",
    "# Sort the DataFrame by the maximum value of AUC (validation)\n",
    "AUCval_df = df.sort_values(by='AUC_valid', ascending=False)\n",
    "AUCval_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>H_units</th>\n",
       "      <th>L2_reg</th>\n",
       "      <th>dropout_rate</th>\n",
       "      <th>AUC_valid</th>\n",
       "      <th>AUC_valid_PDB</th>\n",
       "      <th>AUC_valid_af2</th>\n",
       "      <th>AUC_test</th>\n",
       "      <th>AUC_test_PDB</th>\n",
       "      <th>AUC_test_af2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2_unit</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.782424</td>\n",
       "      <td>0.792101</td>\n",
       "      <td>0.772664</td>\n",
       "      <td>0.787002</td>\n",
       "      <td>0.791250</td>\n",
       "      <td>0.782949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2_unit</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.780724</td>\n",
       "      <td>0.788246</td>\n",
       "      <td>0.774098</td>\n",
       "      <td>0.785832</td>\n",
       "      <td>0.789533</td>\n",
       "      <td>0.780712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>5_unit</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.787927</td>\n",
       "      <td>0.795865</td>\n",
       "      <td>0.780231</td>\n",
       "      <td>0.785527</td>\n",
       "      <td>0.788068</td>\n",
       "      <td>0.784689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2_unit</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.781407</td>\n",
       "      <td>0.788887</td>\n",
       "      <td>0.774123</td>\n",
       "      <td>0.785388</td>\n",
       "      <td>0.788346</td>\n",
       "      <td>0.780716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1_unit</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.784163</td>\n",
       "      <td>0.794051</td>\n",
       "      <td>0.774353</td>\n",
       "      <td>0.785303</td>\n",
       "      <td>0.789975</td>\n",
       "      <td>0.781275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1_unit</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.783704</td>\n",
       "      <td>0.793639</td>\n",
       "      <td>0.773777</td>\n",
       "      <td>0.784845</td>\n",
       "      <td>0.789834</td>\n",
       "      <td>0.780397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>5_unit</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.787058</td>\n",
       "      <td>0.795293</td>\n",
       "      <td>0.779181</td>\n",
       "      <td>0.784023</td>\n",
       "      <td>0.790563</td>\n",
       "      <td>0.779422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2_unit</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.783479</td>\n",
       "      <td>0.791273</td>\n",
       "      <td>0.775401</td>\n",
       "      <td>0.783840</td>\n",
       "      <td>0.789226</td>\n",
       "      <td>0.779383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>5_unit</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.788135</td>\n",
       "      <td>0.796903</td>\n",
       "      <td>0.779455</td>\n",
       "      <td>0.783808</td>\n",
       "      <td>0.789620</td>\n",
       "      <td>0.778691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>10_unit</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.791244</td>\n",
       "      <td>0.798602</td>\n",
       "      <td>0.783384</td>\n",
       "      <td>0.783574</td>\n",
       "      <td>0.790344</td>\n",
       "      <td>0.776481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_unit</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.783874</td>\n",
       "      <td>0.793598</td>\n",
       "      <td>0.774237</td>\n",
       "      <td>0.783571</td>\n",
       "      <td>0.788062</td>\n",
       "      <td>0.779814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2_unit</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.779430</td>\n",
       "      <td>0.786542</td>\n",
       "      <td>0.771830</td>\n",
       "      <td>0.783045</td>\n",
       "      <td>0.786057</td>\n",
       "      <td>0.778242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2_unit</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.782278</td>\n",
       "      <td>0.789517</td>\n",
       "      <td>0.775216</td>\n",
       "      <td>0.782123</td>\n",
       "      <td>0.786727</td>\n",
       "      <td>0.776635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>5_unit</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.785500</td>\n",
       "      <td>0.793300</td>\n",
       "      <td>0.777677</td>\n",
       "      <td>0.782006</td>\n",
       "      <td>0.788225</td>\n",
       "      <td>0.777999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2_unit</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.779394</td>\n",
       "      <td>0.786868</td>\n",
       "      <td>0.771915</td>\n",
       "      <td>0.781970</td>\n",
       "      <td>0.786481</td>\n",
       "      <td>0.778726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2_unit</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.782847</td>\n",
       "      <td>0.793703</td>\n",
       "      <td>0.771687</td>\n",
       "      <td>0.781672</td>\n",
       "      <td>0.788887</td>\n",
       "      <td>0.774680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>10_unit</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.789449</td>\n",
       "      <td>0.797259</td>\n",
       "      <td>0.782122</td>\n",
       "      <td>0.781314</td>\n",
       "      <td>0.787629</td>\n",
       "      <td>0.776113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>5_unit</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.777210</td>\n",
       "      <td>0.785160</td>\n",
       "      <td>0.769038</td>\n",
       "      <td>0.781265</td>\n",
       "      <td>0.785137</td>\n",
       "      <td>0.777047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>5_unit</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.775441</td>\n",
       "      <td>0.784344</td>\n",
       "      <td>0.766484</td>\n",
       "      <td>0.780940</td>\n",
       "      <td>0.785298</td>\n",
       "      <td>0.776823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>10_unit</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.777049</td>\n",
       "      <td>0.785221</td>\n",
       "      <td>0.768735</td>\n",
       "      <td>0.780854</td>\n",
       "      <td>0.785323</td>\n",
       "      <td>0.775883</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     H_units  L2_reg  dropout_rate  AUC_valid  AUC_valid_PDB  AUC_valid_af2  \\\n",
       "48    2_unit  0.1000           0.0   0.782424       0.792101       0.772664   \n",
       "41    2_unit  0.0010           0.5   0.780724       0.788246       0.774098   \n",
       "71    5_unit  0.0010           0.5   0.787927       0.795865       0.780231   \n",
       "40    2_unit  0.0010           0.4   0.781407       0.788887       0.774123   \n",
       "6     1_unit  0.0010           0.0   0.784163       0.794051       0.774353   \n",
       "12    1_unit  0.0100           0.0   0.783704       0.793639       0.773777   \n",
       "74    5_unit  0.0100           0.2   0.787058       0.795293       0.779181   \n",
       "43    2_unit  0.0100           0.1   0.783479       0.791273       0.775401   \n",
       "76    5_unit  0.0100           0.4   0.788135       0.796903       0.779455   \n",
       "95   10_unit  0.0010           0.5   0.791244       0.798602       0.783384   \n",
       "0     1_unit  0.0001           0.0   0.783874       0.793598       0.774237   \n",
       "35    2_unit  0.0001           0.5   0.779430       0.786542       0.771830   \n",
       "38    2_unit  0.0010           0.2   0.782278       0.789517       0.775216   \n",
       "75    5_unit  0.0100           0.3   0.785500       0.793300       0.777677   \n",
       "34    2_unit  0.0001           0.4   0.779394       0.786868       0.771915   \n",
       "42    2_unit  0.0100           0.0   0.782847       0.793703       0.771687   \n",
       "101  10_unit  0.0100           0.5   0.789449       0.797259       0.782122   \n",
       "109   5_unit  1.0000           0.0   0.777210       0.785160       0.769038   \n",
       "82    5_unit  0.1000           0.4   0.775441       0.784344       0.766484   \n",
       "115  10_unit  1.0000           0.0   0.777049       0.785221       0.768735   \n",
       "\n",
       "     AUC_test  AUC_test_PDB  AUC_test_af2  \n",
       "48   0.787002      0.791250      0.782949  \n",
       "41   0.785832      0.789533      0.780712  \n",
       "71   0.785527      0.788068      0.784689  \n",
       "40   0.785388      0.788346      0.780716  \n",
       "6    0.785303      0.789975      0.781275  \n",
       "12   0.784845      0.789834      0.780397  \n",
       "74   0.784023      0.790563      0.779422  \n",
       "43   0.783840      0.789226      0.779383  \n",
       "76   0.783808      0.789620      0.778691  \n",
       "95   0.783574      0.790344      0.776481  \n",
       "0    0.783571      0.788062      0.779814  \n",
       "35   0.783045      0.786057      0.778242  \n",
       "38   0.782123      0.786727      0.776635  \n",
       "75   0.782006      0.788225      0.777999  \n",
       "34   0.781970      0.786481      0.778726  \n",
       "42   0.781672      0.788887      0.774680  \n",
       "101  0.781314      0.787629      0.776113  \n",
       "109  0.781265      0.785137      0.777047  \n",
       "82   0.780940      0.785298      0.776823  \n",
       "115  0.780854      0.785323      0.775883  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort the DataFrame by the maximum value of AUC (test)\n",
    "AUCtest_df = df.sort_values(by='AUC_test', ascending=False)\n",
    "AUCtest_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature extraction from the linear model: check the 20 highest absolute value weights\n",
    "\n",
    "- Also save all the weights before and after training the linear neural network and do the histogram plot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "98d96843242ea63546b941eb98e2ac308b462277a7c4020ae14ccf41d9fe0ade"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
