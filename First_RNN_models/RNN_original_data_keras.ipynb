{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. RNN for the original whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\minij\\anaconda3\\envs\\special_course\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import of required libraries and functions from 'make_dataset' script\n",
    "import os\n",
    "from make_dataset import Discotope_Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tensorflow and keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_load(name_set, data_dir, separate=False):\n",
    "    \n",
    "    '''\n",
    "    Function to load training, validation or test data from the folder/directory you have storaged the whole dataset\n",
    "        - 'name_set': text variable with the type of data set to load ('train', 'test', 'valid')\n",
    "        - 'data_dir': directory where you have storaged the data set (in my case is '../Data/')\n",
    "        - 'separate': boolean that returns the PDBs and AF2 sets separately (only when True)\n",
    "    \n",
    "    Output:\n",
    "        - 'X_all': all observations from both solved and predicted structures all together\n",
    "        - 'y_all': all labels from both solved and predicted structures all together\n",
    "        - 'N_solved': number of observations from solved structures (in case of wanted to unmerge the 'all' array)\n",
    "        - 'N_af2': number of observations from predicted structures (in case of wanted to unmerge the 'all' array)\n",
    "    '''\n",
    "    \n",
    "    # 'Pathlib module' load to work with windows path\n",
    "    import pathlib\n",
    "    temp = pathlib.PosixPath\n",
    "    pathlib.PosixPath = pathlib.WindowsPath\n",
    "    \n",
    "    # List of all subdirectories inside the data directory\n",
    "    dirs = [d for d in os.listdir(data_dir)]\n",
    "    \n",
    "    # 're' module to use regex for filtering the specific directories according to the type of data set to load\n",
    "    pattern = '.*' + name_set + '.*'\n",
    "    R = re.compile(pattern)\n",
    "    filtered = [folder for folder in dirs if R.match(folder)]\n",
    "    \n",
    "    # Loading the data set for solved structures and AlphaFold2 predicted structures\n",
    "    path_af2 = data_dir + filtered[0] + '/dataset.pt'\n",
    "    path_solved = data_dir + filtered[1] + '/dataset.pt'\n",
    "    set_af2 = torch.load(path_af2)\n",
    "    set_solved = torch.load(path_solved)\n",
    "    \n",
    "    # Stack all features and targets to one big array\n",
    "    X_set_solved = np.concatenate([sample[\"X_arr\"] for sample in set_solved])\n",
    "    y_set_solved = np.concatenate([sample[\"y_arr\"] for sample in set_solved])\n",
    "    X_set_af2 = np.concatenate([sample[\"X_arr\"] for sample in set_af2])\n",
    "    y_set_af2 = np.concatenate([sample[\"y_arr\"] for sample in set_af2])\n",
    "    \n",
    "    # NÂº of observations for each subtype of data set\n",
    "    N_set_solved = X_set_solved.shape[0]\n",
    "    N_set_af2 = X_set_af2.shape[0]\n",
    "    \n",
    "    # Stack all features and targets from solved and predicted structures into only one big\n",
    "    X_set_all = np.concatenate((X_set_solved, X_set_af2), axis=0)\n",
    "    y_set_all = np.concatenate((y_set_solved, y_set_af2), axis=0)\n",
    "    \n",
    "    if (separate==True):\n",
    "        return(X_set_all, y_set_all, X_set_solved, y_set_solved, X_set_af2, y_set_af2)\n",
    "    else:\n",
    "        return(X_set_all, y_set_all, N_set_solved, N_set_af2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_load(name_set, data_dir):\n",
    "    \n",
    "    '''\n",
    "    Function to load training, validation or test dataframes from the folder/directory you have storaged the whole dataset.\n",
    "    This function is specifically to have the original dataframes of the data, and their corresponding description\n",
    "        - 'name_set': text variable with the type of data set to load ('train', 'test', 'valid')\n",
    "        - 'data_dir': directory where you have storaged the data set (in my case is '../Data/')\n",
    "    \n",
    "    Output:\n",
    "        - 'set_af2': dataframe for AF2 predicted structures\n",
    "        - 'set_solved': dataframe for PDB solved structures\n",
    "    '''\n",
    "    \n",
    "    # 'Pathlib module' load to work with windows path\n",
    "    import pathlib\n",
    "    temp = pathlib.PosixPath\n",
    "    pathlib.PosixPath = pathlib.WindowsPath\n",
    "    \n",
    "    # List of all subdirectories inside the data directory\n",
    "    dirs = [d for d in os.listdir(data_dir)]\n",
    "    \n",
    "    # 're' module to use regex for filtering the specific directories according to the type of data set to load\n",
    "    pattern = '.*' + name_set + '.*'\n",
    "    R = re.compile(pattern)\n",
    "    filtered = [folder for folder in dirs if R.match(folder)]\n",
    "    \n",
    "    # Loading the data set for solved structures and AlphaFold2 predicted structures\n",
    "    path_af2 = data_dir + filtered[0] + '/dataset.pt'\n",
    "    path_solved = data_dir + filtered[1] + '/dataset.pt'\n",
    "    set_af2 = torch.load(path_af2)\n",
    "    set_solved = torch.load(path_solved)\n",
    "    \n",
    "    return(set_af2, set_solved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_NaN(data, y):\n",
    "    \n",
    "    '''\n",
    "    Function to remove NaN values (some PDB entries have RSA NaN values)\n",
    "        - 'data': numpy array with the specific (train, valid, test) data\n",
    "        - 'y': numpy array with the specific (train, valid, test) labels\n",
    "    \n",
    "    Output:\n",
    "        - 'data_noNaN': array withouth the entries/observations that contain NaN values\n",
    "    '''\n",
    "    \n",
    "    # Merging X and y arrays all together\n",
    "    joint_data = np.hstack((data, y.reshape(-1, 1)))\n",
    "    \n",
    "    # Removal of NaN entries\n",
    "    nan_rows = np.isnan(joint_data).any(axis=1)\n",
    "    data_noNaN = joint_data[~nan_rows, :]\n",
    "    \n",
    "    # Demerging the final array into X and y\n",
    "    X_noNaN = data_noNaN[:, 0:data.shape[1]]\n",
    "    y_noNaN = data_noNaN[:,-1]\n",
    "    \n",
    "    return(X_noNaN, y_noNaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_train(X):\n",
    "    \n",
    "    '''\n",
    "    Function to normalize the columns 532 (pLLDT) and 533 (length) because they have high length\n",
    "        - 'X': data to normalize\n",
    "    '''\n",
    "    \n",
    "    # Create a copy of the X vector to do the normalization\n",
    "    X_scaled = X.copy()\n",
    "    \n",
    "    # Create an instance of MinMaxScaler\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Fit the scaler to the data \n",
    "    #scaler.fit(X_scaled[:, 532:534])\n",
    "    scaler.fit(X_scaled)\n",
    "    \n",
    "    # Transform the data\n",
    "    #X_scaled[:, 532:534] = scaler.fit_transform(X_scaled[:, 532:534])\n",
    "    X_scaled = scaler.fit_transform(X_scaled)\n",
    "    return(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Z_transform_train(X):\n",
    "    \n",
    "    '''\n",
    "    Function to normalize the columns 532 (pLLDT) and 533 (length) because they have high length\n",
    "        - 'X': data to normalize\n",
    "    '''\n",
    "    \n",
    "    # Obtain the mean and standard deviation for each feature on the array\n",
    "    X_mean = np.mean(X, axis=0)\n",
    "    X_std = np.std(X, axis=0)\n",
    "    \n",
    "    # Z-transform (standardization)\n",
    "    X_scaled = (X - X_mean)/X_std\n",
    "    return(X_scaled, X_mean, X_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Z_transform_valid(X, mean_train, sd_train):\n",
    "    \n",
    "    '''\n",
    "    Function to normalize the columns 532 (pLLDT) and 533 (length) because they have high length\n",
    "        - 'X': data to normalize\n",
    "        - 'mean_train': mean from standardized training set\n",
    "        - 'sd_train': standard deviation from standardized training set\n",
    "    '''\n",
    "    \n",
    "    # Z-transform (standardization)\n",
    "    X_scaled = (X - mean_train)/sd_train\n",
    "    return(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_weight_calculator(y_train):\n",
    "        \n",
    "    '''\n",
    "    Function to calculate the class weights for the unbalanced data\n",
    "        - 'y_train': training labels (contains 0 and 1)\n",
    "    '''\n",
    "    \n",
    "    # Compute the class weights with sklearn function\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "\n",
    "    # Convert the class weights to a dictionary\n",
    "    class_weight = dict(enumerate(class_weights))\n",
    "    return(class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neuronal network model with one layer\n",
    "def nn_model(train_data, y_train, valid_data, y_valid, loss_fun, alpha, class_weight_fn):\n",
    "    \n",
    "    '''\n",
    "    Function to create and train/validate the feed-forward neuronal network with only 1 hidden layer\n",
    "        -'train_data': X train standardized\n",
    "        -'y_train': training labels\n",
    "        -'valid_data': X validation standardized\n",
    "        -'y_valid': validation labels\n",
    "        -'act_fun': activation function\n",
    "        -'loss_fun': loss function\n",
    "        -'class_weight_calculator': function to calculate the weights for each class\n",
    "    \n",
    "    Output:\n",
    "        -'model': neural network model trained\n",
    "        -'history': attributes obtained during fitting the model\n",
    "    '''\n",
    "    \n",
    "    # Calculation of the class weights with function previously defined\n",
    "    class_weight = class_weight_fn(y_train)\n",
    "    \n",
    "    # Normalization of the class_weight to sum 1\n",
    "    tot = class_weight[0] + class_weight[1]\n",
    "    class_weight[0] = class_weight[0]/tot\n",
    "    class_weight[1] = class_weight[1]/tot\n",
    "    \n",
    "    # Implementation of keras for creating a sequential model with 1 layer\n",
    "    tf.random.set_seed(1234)\n",
    "    from keras.layers import Dense, Dropout\n",
    "    from keras import regularizers, metrics\n",
    "    \n",
    "    model = keras.Sequential()\n",
    "    # Input layer with train_data.shape neurons and a hidden layer with 1 neuron\n",
    "    model.add(Dense(1, activation='relu', input_shape=train_data.shape[1:], kernel_regularizer=regularizers.l2(alpha)))\n",
    "    # Output layer with sigmoid activation (better for binary classification)\n",
    "    model.add(Dense(1, activation='sigmoid', kernel_regularizer=regularizers.l2(alpha)))\n",
    "    \n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    model.compile(optimizer=opt, loss=loss_fun, metrics=\n",
    "                  ['accuracy', metrics.Precision(), metrics.Recall(), metrics.AUC(), loss_fun])\n",
    "    \n",
    "    history = model.fit(train_data, y_train, epochs = 50, batch_size=128, verbose=0, class_weight=class_weight, \n",
    "                        validation_data = (valid_data, y_valid))\n",
    "    \n",
    "    return(model, history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_plot(loss_values):\n",
    "    \n",
    "    '''\n",
    "    Function to plot the loss curve of the training of the model\n",
    "        - 'loss_values': array with the loss values for each iteration of the training\n",
    "    '''\n",
    "    \n",
    "    plt.plot(loss_values, label = 'Train')\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ROC_AUC(y_true, y_hat):\n",
    "    \n",
    "    '''\n",
    "    Function to obtain the AUC value based on the ROC curve\n",
    "        - 'y_true': y original values\n",
    "        - 'y_hat': y predicted values\n",
    "        \n",
    "    Output:\n",
    "        - 'ROC_auc': AUC value\n",
    "    '''\n",
    "    \n",
    "    fpr, tpr, _ = metrics.roc_curve(y_true, y_hat)\n",
    "    ROC_auc = metrics.auc(fpr, tpr)\n",
    "    return(ROC_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PR_AUC(y_true, y_hat):\n",
    "    \n",
    "    '''\n",
    "    Function to obtain the AUC value based on the precision and recall parameters\n",
    "        - 'y_true': y original values\n",
    "        - 'y_hat': y predicted values\n",
    "        \n",
    "    Output:\n",
    "        - 'PR_auc': AUC value\n",
    "    '''\n",
    "    \n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y_true, y_hat)\n",
    "    PR_auc = metrics.auc(recall, precision)\n",
    "    return(PR_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epitope rank percentile score\n",
    "# https://github.com/Magnushhoie/discotope3/blob/main/src/models/mlscripts.py#L55\n",
    "\n",
    "def get_percentile_score_arr(\n",
    "    scores: np.array,\n",
    "    epitopes: np.array,\n",
    "):\n",
    "    \n",
    "    \"\"\"Find mean predicted epitope rank percentile score from the scores (y_hat) and the epitopes (y_true)\"\"\"\n",
    "    epitopes_bool = epitopes.astype(bool)\n",
    "    assert epitopes_bool.dtype == \"bool\"\n",
    "\n",
    "    c = scores[epitopes_bool].mean()\n",
    "    c_percentile = (c > scores).mean()\n",
    "\n",
    "    return c_percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((480297, 536), (480297,), (119792, 536), (119792,), (14084, 536), (14084,))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data loading for training, validation, and test data sets (needs a couple of minutes)\n",
    "X_train, y_train, X_train_PDB, y_train_PDB, X_train_af2, y_train_af2 = data_load(name_set='train', data_dir='../Data/', separate = True)\n",
    "X_valid, y_valid, X_valid_PDB, y_valid_PDB, X_valid_af2, y_valid_af2 = data_load(name_set='valid', data_dir='../Data/', separate = True)\n",
    "X_test, y_test, X_test_PDB, y_test_PDB, X_test_af2, y_test_af2 = data_load(name_set='test', data_dir='../Data/', separate = True)\n",
    "\n",
    "X_train.shape, y_train.shape, X_valid.shape, y_valid.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((463580, 536), (463580,), (118084, 536), (118084,), (12983, 536), (12983,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data manipulation to remove all PDB entries with NaN values in the RSA feature (535)\n",
    "X_train, y_train = remove_NaN(data=X_train, y=y_train)\n",
    "X_valid, y_valid = remove_NaN(data=X_valid, y=y_valid)\n",
    "X_test, y_test = remove_NaN(data=X_test, y=y_test)\n",
    "\n",
    "X_train.shape, y_train.shape, X_valid.shape, y_valid.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((463580, 536), (118084, 536), (12983, 536))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data normalization of features 532 (pLLDT) and 533 (length)\n",
    "X_train_sc, mean_X_train, sd_X_train = Z_transform_train(X_train)\n",
    "X_valid_sc = Z_transform_valid(X_valid, mean_X_train, sd_X_train)\n",
    "X_test_sc = Z_transform_valid(X_test, mean_X_train, sd_X_train)\n",
    "\n",
    "X_train_sc.shape, X_valid_sc.shape, X_test_sc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class count (train):\n",
      "\n",
      "      Total: 463580\n",
      "      Epitope label: 42458 (9.16% of total)\n"
     ]
    }
   ],
   "source": [
    "# Class unbalanced in train\n",
    "zero, one = np.bincount(y_train.astype(int))\n",
    "total = zero + one\n",
    "print(\"Class count (train):\\n\\n      Total: {}\\n      Epitope label: {} ({:.2f}% of total)\".format(total, one, 100*one/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class count (validation):\n",
      "\n",
      "      Total: 118084\n",
      "      Epitope label: 9810 (8.31% of total)\n"
     ]
    }
   ],
   "source": [
    "# Class unbalanced in valid\n",
    "zero, one = np.bincount(y_valid.astype(int))\n",
    "total = zero + one\n",
    "print(\"Class count (validation):\\n\\n      Total: {}\\n      Epitope label: {} ({:.2f}% of total)\".format(total, one, 100*one/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class count (test):\n",
      "\n",
      "      Total: 12983\n",
      "      Epitope label: 792 (6.10% of total)\n"
     ]
    }
   ],
   "source": [
    "# Class unbalanced in train\n",
    "zero, one = np.bincount(y_test.astype(int))\n",
    "total = zero + one\n",
    "print(\"Class count (test):\\n\\n      Total: {}\\n      Epitope label: {} ({:.2f}% of total)\".format(total, one, 100*one/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Parameters definition\n",
    "loss = 'binary_crossentropy'\n",
    "l2_value = 0.01\n",
    "\n",
    "# Training and validation of the model\n",
    "model, history = nn_model(train_data=X_train_sc, y_train=y_train, valid_data=X_valid_sc, y_valid=y_valid, \n",
    "                          loss_fun=loss, alpha=l2_value, class_weight_fn=class_weight_calculator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Summarize history for loss\n",
    "plt.plot(history.history['loss'], label = 'train')\n",
    "plt.plot(history.history['val_loss'], label='validation')\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3691/3691 [==============================] - 8s 2ms/step - loss: 0.5749 - accuracy: 0.6752 - precision_2: 0.1676 - recall_2: 0.7339 - auc_2: 0.7755 - binary_crossentropy: 0.5670\n",
      "loss : 0.5748986601829529\n",
      "accuracy : 0.6751719117164612\n",
      "precision_2 : 0.16764849424362183\n",
      "recall_2 : 0.7339449524879456\n",
      "auc_2 : 0.7754640579223633\n",
      "binary_crossentropy : 0.5669654011726379\n"
     ]
    }
   ],
   "source": [
    "# Metrics from the model\n",
    "metrics_names = model.metrics_names\n",
    "metrics_values = model.evaluate(X_valid_sc, y_valid)\n",
    "\n",
    "for name, value in zip(metrics_names, metrics_values):\n",
    "    print(name, ':', value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3691/3691 [==============================] - 6s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# Obtaining probabilities values\n",
    "y_pred_valid_prob = model.predict(X_valid_sc)\n",
    "\n",
    "# Conversion to 0 or 1 labels (0.5 threshold)\n",
    "y_pred_valid = (y_pred_valid_prob > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(108274, 75137)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count of '0' in the valid and the prediction\n",
    "np.count_nonzero(y_valid == 0), np.count_nonzero(y_pred_valid == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9810, 42947)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count of '1' in the valid and the prediction\n",
    "np.count_nonzero(y_valid == 1), np.count_nonzero(y_pred_valid == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [0],\n",
       "       [0],\n",
       "       ...,\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid accuracy: 0.67517\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model's performance\n",
    "accuracy_valid = accuracy_score(y_valid, y_pred_valid)\n",
    "print(f'Valid accuracy: {accuracy_valid:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[72527, 35747],\n",
       "       [ 2610,  7200]], dtype=int64)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_valid, y_pred_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ROC)AUC value for the validation data: 0.776\n",
      "(PR)AUC value for the validation data: 0.237\n"
     ]
    }
   ],
   "source": [
    "# AUC value (valid)\n",
    "ROC_AUC_valid = ROC_AUC(y_valid, y_pred_valid_prob)\n",
    "PR_AUC_valid = PR_AUC(y_valid, y_pred_valid_prob)\n",
    "print(f'(ROC)AUC value for the validation data: {ROC_AUC_valid:.3f}')\n",
    "print(f'(PR)AUC value for the validation data: {PR_AUC_valid:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AUC: 0.7755062881110872\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "auc_valid = roc_auc_score(y_valid, y_pred_valid_prob)\n",
    "print('Validation AUC:', auc_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6363012770570102"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Epitope rank (valid)\n",
    "c_percentile_valid = get_percentile_score_arr(scores = y_pred_valid, epitopes = y_valid)\n",
    "c_percentile_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False positive ratio: 0.30272517868635884\n"
     ]
    }
   ],
   "source": [
    "print('False positive ratio:', confusion_matrix(y_valid, y_pred_valid)[0][1]/len(y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>118084.000000</td>\n",
       "      <td>118084.000000</td>\n",
       "      <td>118084.000000</td>\n",
       "      <td>118084.000000</td>\n",
       "      <td>118084.000000</td>\n",
       "      <td>118084.000000</td>\n",
       "      <td>118084.000000</td>\n",
       "      <td>118084.000000</td>\n",
       "      <td>118084.000000</td>\n",
       "      <td>118084.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>118084.000000</td>\n",
       "      <td>118084.000000</td>\n",
       "      <td>118084.000000</td>\n",
       "      <td>118084.000000</td>\n",
       "      <td>118084.000000</td>\n",
       "      <td>118084.000000</td>\n",
       "      <td>118084.000000</td>\n",
       "      <td>118084.000000</td>\n",
       "      <td>118084.000000</td>\n",
       "      <td>118084.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.030735</td>\n",
       "      <td>-0.006413</td>\n",
       "      <td>-0.016117</td>\n",
       "      <td>0.008816</td>\n",
       "      <td>-0.027986</td>\n",
       "      <td>0.032439</td>\n",
       "      <td>-0.008042</td>\n",
       "      <td>0.003230</td>\n",
       "      <td>-0.013860</td>\n",
       "      <td>0.010028</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013225</td>\n",
       "      <td>-0.006994</td>\n",
       "      <td>-0.003578</td>\n",
       "      <td>0.006770</td>\n",
       "      <td>0.014987</td>\n",
       "      <td>-0.030313</td>\n",
       "      <td>0.114893</td>\n",
       "      <td>0.044084</td>\n",
       "      <td>-0.021855</td>\n",
       "      <td>-0.030133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.054753</td>\n",
       "      <td>0.982748</td>\n",
       "      <td>0.966172</td>\n",
       "      <td>1.016889</td>\n",
       "      <td>0.935639</td>\n",
       "      <td>1.052854</td>\n",
       "      <td>0.973852</td>\n",
       "      <td>1.006075</td>\n",
       "      <td>0.972854</td>\n",
       "      <td>1.014922</td>\n",
       "      <td>...</td>\n",
       "      <td>1.028846</td>\n",
       "      <td>0.987984</td>\n",
       "      <td>0.993617</td>\n",
       "      <td>1.011418</td>\n",
       "      <td>1.052383</td>\n",
       "      <td>0.924405</td>\n",
       "      <td>0.817387</td>\n",
       "      <td>0.973777</td>\n",
       "      <td>1.000559</td>\n",
       "      <td>0.999383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.253525</td>\n",
       "      <td>-0.181479</td>\n",
       "      <td>-0.230336</td>\n",
       "      <td>-0.243042</td>\n",
       "      <td>-0.215547</td>\n",
       "      <td>-0.273908</td>\n",
       "      <td>-0.152373</td>\n",
       "      <td>-0.248631</td>\n",
       "      <td>-0.244214</td>\n",
       "      <td>-0.302159</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.214933</td>\n",
       "      <td>-0.271648</td>\n",
       "      <td>-0.261953</td>\n",
       "      <td>-0.272444</td>\n",
       "      <td>-0.136528</td>\n",
       "      <td>-0.201199</td>\n",
       "      <td>-7.042226</td>\n",
       "      <td>-1.553949</td>\n",
       "      <td>-1.036988</td>\n",
       "      <td>-1.183899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.253525</td>\n",
       "      <td>-0.181479</td>\n",
       "      <td>-0.230336</td>\n",
       "      <td>-0.243042</td>\n",
       "      <td>-0.215547</td>\n",
       "      <td>-0.273908</td>\n",
       "      <td>-0.152373</td>\n",
       "      <td>-0.248631</td>\n",
       "      <td>-0.244214</td>\n",
       "      <td>-0.302159</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.214933</td>\n",
       "      <td>-0.271648</td>\n",
       "      <td>-0.261953</td>\n",
       "      <td>-0.272444</td>\n",
       "      <td>-0.136528</td>\n",
       "      <td>-0.201199</td>\n",
       "      <td>0.081265</td>\n",
       "      <td>-0.701550</td>\n",
       "      <td>-1.036988</td>\n",
       "      <td>-0.977368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.253525</td>\n",
       "      <td>-0.181479</td>\n",
       "      <td>-0.230336</td>\n",
       "      <td>-0.243042</td>\n",
       "      <td>-0.215547</td>\n",
       "      <td>-0.273908</td>\n",
       "      <td>-0.152373</td>\n",
       "      <td>-0.248631</td>\n",
       "      <td>-0.244214</td>\n",
       "      <td>-0.302159</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.214933</td>\n",
       "      <td>-0.271648</td>\n",
       "      <td>-0.261953</td>\n",
       "      <td>-0.272444</td>\n",
       "      <td>-0.136528</td>\n",
       "      <td>-0.201199</td>\n",
       "      <td>0.409537</td>\n",
       "      <td>0.084771</td>\n",
       "      <td>0.964331</td>\n",
       "      <td>-0.185783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-0.253525</td>\n",
       "      <td>-0.181479</td>\n",
       "      <td>-0.230336</td>\n",
       "      <td>-0.243042</td>\n",
       "      <td>-0.215547</td>\n",
       "      <td>-0.273908</td>\n",
       "      <td>-0.152373</td>\n",
       "      <td>-0.248631</td>\n",
       "      <td>-0.244214</td>\n",
       "      <td>-0.302159</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.214933</td>\n",
       "      <td>-0.271648</td>\n",
       "      <td>-0.261953</td>\n",
       "      <td>-0.272444</td>\n",
       "      <td>-0.136528</td>\n",
       "      <td>-0.201199</td>\n",
       "      <td>0.521535</td>\n",
       "      <td>0.679468</td>\n",
       "      <td>0.964331</td>\n",
       "      <td>0.696113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.944389</td>\n",
       "      <td>5.510285</td>\n",
       "      <td>4.341478</td>\n",
       "      <td>4.114523</td>\n",
       "      <td>4.639350</td>\n",
       "      <td>3.650864</td>\n",
       "      <td>6.562829</td>\n",
       "      <td>4.022018</td>\n",
       "      <td>4.094769</td>\n",
       "      <td>3.309512</td>\n",
       "      <td>...</td>\n",
       "      <td>4.652612</td>\n",
       "      <td>3.681229</td>\n",
       "      <td>3.817473</td>\n",
       "      <td>3.670477</td>\n",
       "      <td>7.324488</td>\n",
       "      <td>4.970215</td>\n",
       "      <td>0.521535</td>\n",
       "      <td>3.580266</td>\n",
       "      <td>0.964331</td>\n",
       "      <td>4.776337</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0              1              2              3   \\\n",
       "count  118084.000000  118084.000000  118084.000000  118084.000000   \n",
       "mean        0.030735      -0.006413      -0.016117       0.008816   \n",
       "std         1.054753       0.982748       0.966172       1.016889   \n",
       "min        -0.253525      -0.181479      -0.230336      -0.243042   \n",
       "25%        -0.253525      -0.181479      -0.230336      -0.243042   \n",
       "50%        -0.253525      -0.181479      -0.230336      -0.243042   \n",
       "75%        -0.253525      -0.181479      -0.230336      -0.243042   \n",
       "max         3.944389       5.510285       4.341478       4.114523   \n",
       "\n",
       "                  4              5              6              7   \\\n",
       "count  118084.000000  118084.000000  118084.000000  118084.000000   \n",
       "mean       -0.027986       0.032439      -0.008042       0.003230   \n",
       "std         0.935639       1.052854       0.973852       1.006075   \n",
       "min        -0.215547      -0.273908      -0.152373      -0.248631   \n",
       "25%        -0.215547      -0.273908      -0.152373      -0.248631   \n",
       "50%        -0.215547      -0.273908      -0.152373      -0.248631   \n",
       "75%        -0.215547      -0.273908      -0.152373      -0.248631   \n",
       "max         4.639350       3.650864       6.562829       4.022018   \n",
       "\n",
       "                  8              9   ...             14             15  \\\n",
       "count  118084.000000  118084.000000  ...  118084.000000  118084.000000   \n",
       "mean       -0.013860       0.010028  ...       0.013225      -0.006994   \n",
       "std         0.972854       1.014922  ...       1.028846       0.987984   \n",
       "min        -0.244214      -0.302159  ...      -0.214933      -0.271648   \n",
       "25%        -0.244214      -0.302159  ...      -0.214933      -0.271648   \n",
       "50%        -0.244214      -0.302159  ...      -0.214933      -0.271648   \n",
       "75%        -0.244214      -0.302159  ...      -0.214933      -0.271648   \n",
       "max         4.094769       3.309512  ...       4.652612       3.681229   \n",
       "\n",
       "                  16             17             18             19  \\\n",
       "count  118084.000000  118084.000000  118084.000000  118084.000000   \n",
       "mean       -0.003578       0.006770       0.014987      -0.030313   \n",
       "std         0.993617       1.011418       1.052383       0.924405   \n",
       "min        -0.261953      -0.272444      -0.136528      -0.201199   \n",
       "25%        -0.261953      -0.272444      -0.136528      -0.201199   \n",
       "50%        -0.261953      -0.272444      -0.136528      -0.201199   \n",
       "75%        -0.261953      -0.272444      -0.136528      -0.201199   \n",
       "max         3.817473       3.670477       7.324488       4.970215   \n",
       "\n",
       "                  20             21             22             23  \n",
       "count  118084.000000  118084.000000  118084.000000  118084.000000  \n",
       "mean        0.114893       0.044084      -0.021855      -0.030133  \n",
       "std         0.817387       0.973777       1.000559       0.999383  \n",
       "min        -7.042226      -1.553949      -1.036988      -1.183899  \n",
       "25%         0.081265      -0.701550      -1.036988      -0.977368  \n",
       "50%         0.409537       0.084771       0.964331      -0.185783  \n",
       "75%         0.521535       0.679468       0.964331       0.696113  \n",
       "max         0.521535       3.580266       0.964331       4.776337  \n",
       "\n",
       "[8 rows x 24 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X_valid_sc[:, 512:]).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.253525</td>\n",
       "      <td>-0.181479</td>\n",
       "      <td>-0.230336</td>\n",
       "      <td>-0.243042</td>\n",
       "      <td>-0.215547</td>\n",
       "      <td>-0.273908</td>\n",
       "      <td>-0.152373</td>\n",
       "      <td>-0.248631</td>\n",
       "      <td>-0.244214</td>\n",
       "      <td>-0.302159</td>\n",
       "      <td>...</td>\n",
       "      <td>4.652612</td>\n",
       "      <td>-0.271648</td>\n",
       "      <td>-0.261953</td>\n",
       "      <td>-0.272444</td>\n",
       "      <td>-0.136528</td>\n",
       "      <td>-0.201199</td>\n",
       "      <td>0.521535</td>\n",
       "      <td>0.672860</td>\n",
       "      <td>-1.036988</td>\n",
       "      <td>1.219883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.253525</td>\n",
       "      <td>-0.181479</td>\n",
       "      <td>4.341478</td>\n",
       "      <td>-0.243042</td>\n",
       "      <td>-0.215547</td>\n",
       "      <td>-0.273908</td>\n",
       "      <td>-0.152373</td>\n",
       "      <td>-0.248631</td>\n",
       "      <td>-0.244214</td>\n",
       "      <td>-0.302159</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.214933</td>\n",
       "      <td>-0.271648</td>\n",
       "      <td>-0.261953</td>\n",
       "      <td>-0.272444</td>\n",
       "      <td>-0.136528</td>\n",
       "      <td>-0.201199</td>\n",
       "      <td>0.521535</td>\n",
       "      <td>0.672860</td>\n",
       "      <td>-1.036988</td>\n",
       "      <td>1.256222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.253525</td>\n",
       "      <td>-0.181479</td>\n",
       "      <td>-0.230336</td>\n",
       "      <td>-0.243042</td>\n",
       "      <td>4.639350</td>\n",
       "      <td>-0.273908</td>\n",
       "      <td>-0.152373</td>\n",
       "      <td>-0.248631</td>\n",
       "      <td>-0.244214</td>\n",
       "      <td>-0.302159</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.214933</td>\n",
       "      <td>-0.271648</td>\n",
       "      <td>-0.261953</td>\n",
       "      <td>-0.272444</td>\n",
       "      <td>-0.136528</td>\n",
       "      <td>-0.201199</td>\n",
       "      <td>0.521535</td>\n",
       "      <td>0.672860</td>\n",
       "      <td>-1.036988</td>\n",
       "      <td>-0.554929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.253525</td>\n",
       "      <td>-0.181479</td>\n",
       "      <td>-0.230336</td>\n",
       "      <td>-0.243042</td>\n",
       "      <td>-0.215547</td>\n",
       "      <td>-0.273908</td>\n",
       "      <td>-0.152373</td>\n",
       "      <td>-0.248631</td>\n",
       "      <td>-0.244214</td>\n",
       "      <td>-0.302159</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.214933</td>\n",
       "      <td>-0.271648</td>\n",
       "      <td>-0.261953</td>\n",
       "      <td>-0.272444</td>\n",
       "      <td>-0.136528</td>\n",
       "      <td>-0.201199</td>\n",
       "      <td>0.521535</td>\n",
       "      <td>0.672860</td>\n",
       "      <td>-1.036988</td>\n",
       "      <td>-0.591440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.253525</td>\n",
       "      <td>-0.181479</td>\n",
       "      <td>-0.230336</td>\n",
       "      <td>-0.243042</td>\n",
       "      <td>-0.215547</td>\n",
       "      <td>-0.273908</td>\n",
       "      <td>-0.152373</td>\n",
       "      <td>-0.248631</td>\n",
       "      <td>-0.244214</td>\n",
       "      <td>-0.302159</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.214933</td>\n",
       "      <td>-0.271648</td>\n",
       "      <td>-0.261953</td>\n",
       "      <td>-0.272444</td>\n",
       "      <td>-0.136528</td>\n",
       "      <td>-0.201199</td>\n",
       "      <td>0.521535</td>\n",
       "      <td>0.672860</td>\n",
       "      <td>-1.036988</td>\n",
       "      <td>0.041361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118079</th>\n",
       "      <td>-0.253525</td>\n",
       "      <td>-0.181479</td>\n",
       "      <td>-0.230336</td>\n",
       "      <td>-0.243042</td>\n",
       "      <td>-0.215547</td>\n",
       "      <td>3.650864</td>\n",
       "      <td>-0.152373</td>\n",
       "      <td>-0.248631</td>\n",
       "      <td>-0.244214</td>\n",
       "      <td>-0.302159</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.214933</td>\n",
       "      <td>-0.271648</td>\n",
       "      <td>-0.261953</td>\n",
       "      <td>-0.272444</td>\n",
       "      <td>-0.136528</td>\n",
       "      <td>-0.201199</td>\n",
       "      <td>-2.917591</td>\n",
       "      <td>-1.177308</td>\n",
       "      <td>0.964331</td>\n",
       "      <td>0.938345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118080</th>\n",
       "      <td>-0.253525</td>\n",
       "      <td>-0.181479</td>\n",
       "      <td>-0.230336</td>\n",
       "      <td>-0.243042</td>\n",
       "      <td>-0.215547</td>\n",
       "      <td>-0.273908</td>\n",
       "      <td>-0.152373</td>\n",
       "      <td>-0.248631</td>\n",
       "      <td>-0.244214</td>\n",
       "      <td>3.309512</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.214933</td>\n",
       "      <td>-0.271648</td>\n",
       "      <td>-0.261953</td>\n",
       "      <td>-0.272444</td>\n",
       "      <td>-0.136528</td>\n",
       "      <td>-0.201199</td>\n",
       "      <td>-3.555789</td>\n",
       "      <td>-1.177308</td>\n",
       "      <td>0.964331</td>\n",
       "      <td>-0.198314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118081</th>\n",
       "      <td>-0.253525</td>\n",
       "      <td>-0.181479</td>\n",
       "      <td>-0.230336</td>\n",
       "      <td>4.114523</td>\n",
       "      <td>-0.215547</td>\n",
       "      <td>-0.273908</td>\n",
       "      <td>-0.152373</td>\n",
       "      <td>-0.248631</td>\n",
       "      <td>-0.244214</td>\n",
       "      <td>-0.302159</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.214933</td>\n",
       "      <td>-0.271648</td>\n",
       "      <td>-0.261953</td>\n",
       "      <td>-0.272444</td>\n",
       "      <td>-0.136528</td>\n",
       "      <td>-0.201199</td>\n",
       "      <td>-4.350400</td>\n",
       "      <td>-1.177308</td>\n",
       "      <td>0.964331</td>\n",
       "      <td>1.618959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118082</th>\n",
       "      <td>-0.253525</td>\n",
       "      <td>-0.181479</td>\n",
       "      <td>-0.230336</td>\n",
       "      <td>-0.243042</td>\n",
       "      <td>-0.215547</td>\n",
       "      <td>-0.273908</td>\n",
       "      <td>-0.152373</td>\n",
       "      <td>-0.248631</td>\n",
       "      <td>-0.244214</td>\n",
       "      <td>-0.302159</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.214933</td>\n",
       "      <td>-0.271648</td>\n",
       "      <td>-0.261953</td>\n",
       "      <td>3.670477</td>\n",
       "      <td>-0.136528</td>\n",
       "      <td>-0.201199</td>\n",
       "      <td>-5.570797</td>\n",
       "      <td>-1.177308</td>\n",
       "      <td>0.964331</td>\n",
       "      <td>1.940300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118083</th>\n",
       "      <td>-0.253525</td>\n",
       "      <td>-0.181479</td>\n",
       "      <td>-0.230336</td>\n",
       "      <td>-0.243042</td>\n",
       "      <td>-0.215547</td>\n",
       "      <td>-0.273908</td>\n",
       "      <td>-0.152373</td>\n",
       "      <td>-0.248631</td>\n",
       "      <td>-0.244214</td>\n",
       "      <td>3.309512</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.214933</td>\n",
       "      <td>-0.271648</td>\n",
       "      <td>-0.261953</td>\n",
       "      <td>-0.272444</td>\n",
       "      <td>-0.136528</td>\n",
       "      <td>-0.201199</td>\n",
       "      <td>-5.818932</td>\n",
       "      <td>-1.177308</td>\n",
       "      <td>0.964331</td>\n",
       "      <td>3.255184</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>118084 rows Ã 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6   \\\n",
       "0      -0.253525 -0.181479 -0.230336 -0.243042 -0.215547 -0.273908 -0.152373   \n",
       "1      -0.253525 -0.181479  4.341478 -0.243042 -0.215547 -0.273908 -0.152373   \n",
       "2      -0.253525 -0.181479 -0.230336 -0.243042  4.639350 -0.273908 -0.152373   \n",
       "3      -0.253525 -0.181479 -0.230336 -0.243042 -0.215547 -0.273908 -0.152373   \n",
       "4      -0.253525 -0.181479 -0.230336 -0.243042 -0.215547 -0.273908 -0.152373   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "118079 -0.253525 -0.181479 -0.230336 -0.243042 -0.215547  3.650864 -0.152373   \n",
       "118080 -0.253525 -0.181479 -0.230336 -0.243042 -0.215547 -0.273908 -0.152373   \n",
       "118081 -0.253525 -0.181479 -0.230336  4.114523 -0.215547 -0.273908 -0.152373   \n",
       "118082 -0.253525 -0.181479 -0.230336 -0.243042 -0.215547 -0.273908 -0.152373   \n",
       "118083 -0.253525 -0.181479 -0.230336 -0.243042 -0.215547 -0.273908 -0.152373   \n",
       "\n",
       "              7         8         9   ...        14        15        16  \\\n",
       "0      -0.248631 -0.244214 -0.302159  ...  4.652612 -0.271648 -0.261953   \n",
       "1      -0.248631 -0.244214 -0.302159  ... -0.214933 -0.271648 -0.261953   \n",
       "2      -0.248631 -0.244214 -0.302159  ... -0.214933 -0.271648 -0.261953   \n",
       "3      -0.248631 -0.244214 -0.302159  ... -0.214933 -0.271648 -0.261953   \n",
       "4      -0.248631 -0.244214 -0.302159  ... -0.214933 -0.271648 -0.261953   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "118079 -0.248631 -0.244214 -0.302159  ... -0.214933 -0.271648 -0.261953   \n",
       "118080 -0.248631 -0.244214  3.309512  ... -0.214933 -0.271648 -0.261953   \n",
       "118081 -0.248631 -0.244214 -0.302159  ... -0.214933 -0.271648 -0.261953   \n",
       "118082 -0.248631 -0.244214 -0.302159  ... -0.214933 -0.271648 -0.261953   \n",
       "118083 -0.248631 -0.244214  3.309512  ... -0.214933 -0.271648 -0.261953   \n",
       "\n",
       "              17        18        19        20        21        22        23  \n",
       "0      -0.272444 -0.136528 -0.201199  0.521535  0.672860 -1.036988  1.219883  \n",
       "1      -0.272444 -0.136528 -0.201199  0.521535  0.672860 -1.036988  1.256222  \n",
       "2      -0.272444 -0.136528 -0.201199  0.521535  0.672860 -1.036988 -0.554929  \n",
       "3      -0.272444 -0.136528 -0.201199  0.521535  0.672860 -1.036988 -0.591440  \n",
       "4      -0.272444 -0.136528 -0.201199  0.521535  0.672860 -1.036988  0.041361  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "118079 -0.272444 -0.136528 -0.201199 -2.917591 -1.177308  0.964331  0.938345  \n",
       "118080 -0.272444 -0.136528 -0.201199 -3.555789 -1.177308  0.964331 -0.198314  \n",
       "118081 -0.272444 -0.136528 -0.201199 -4.350400 -1.177308  0.964331  1.618959  \n",
       "118082  3.670477 -0.136528 -0.201199 -5.570797 -1.177308  0.964331  1.940300  \n",
       "118083 -0.272444 -0.136528 -0.201199 -5.818932 -1.177308  0.964331  3.255184  \n",
       "\n",
       "[118084 rows x 24 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X_valid_sc[:, 512:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- AUC per feature\n",
    "- Change activation and regularization\n",
    "- Balancing VS not balancing\n",
    "- Check feature normalization only in the relevant\n",
    "- Include performance on validation and test\n",
    "- Resampling from the training data or randoms value in the weights (later): ensemble\n",
    "- Save dataframes of train, valid, test with regularization: no categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "98d96843242ea63546b941eb98e2ac308b462277a7c4020ae14ccf41d9fe0ade"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
